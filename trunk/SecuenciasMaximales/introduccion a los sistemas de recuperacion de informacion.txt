http://www.um.es/gtiweb/fjmm/sarisite/tema1.html

AUTOR: FRANCISCO JAVIER MARTÍNEZ MÉNDEZ. Univ. De Murcia

Introducción a los Sistemas de Recuperación de Información

Bibliografía básica: FRAKES, W.B. Introduction to Information Storage and Retrieval Systems. En: FRAKES, W.B. and BAEZA-YATES, R. Information Retrieval: data structures and algorithms. Englewood Cliffs: Prentice Hall, 1992. ISBN 0-132-463837-9. p. 1-12

1.1. Conceptos generales de los Sistemas de Recuperación de Información (SRI). 
La importancia que posee la recuperación de información textual puede inferirse fácilmente del hecho de que en la bibliografía se hace referencia a ella con la amplitud del término "Information Retrieval" y que para los demás tipos de sistemas de recuperación de información se empleen otras denominaciones más específicas. Information Retrieval da nombre a muchos grupos y asociaciones que trabajan sobre el tema, como puede ser el "Special Interest Group on Information Retrieval" de ACM o el "Center for Intelligence Information Retrieval". En la medida que las instituciones han descubierto las potencialidades de las autopistas de la información para hacer accesibles sus bases de datos textuales, ha crecido la importancia de la recuperación de este tipo de información. 
Realmente no se trata de una necesidad nueva: la escritura es probablemente uno de los medios más antiguos de almacenar y transmitir el conocimiento; y a partir de un cierto volumen de texto escrito se hace imprescindible un sistema organizativo que posibilite la localización de la información que se precise en cualquier momento. Esta necesidad ha estado cubierta por técnicas que no han variado en 200 años básicamente hasta que la disponibilidad de ordenadores cada vez más potentes, los dispositivos de almacenamiento más rápidos y de mayor capacidad y las redes de gran ancho de banda han producido una explosión de la información que no puede ser afrontada sin un amplio conjunto de nuevas técnicas de almacenamiento, acceso, interrogación y manipulación de esa información. 
El desarrollo de los sistemas automatizados de recuperación de información se inició con el objetivo de facilitar el manejo de la enorme cantidad de literatura científica surgida desde los años 40. No ha quedado restringida a este campo sino que se ha extendido a otras áreas: cualquier disciplina que base su trabajo en la utilización de documentos puede beneficiarse de las técnicas de recuperación de información textual. En los últimos 30 años se han desarrollado estructuras de datos eficientes para el almacenamiento de índices, sofisticados algoritmos de interrogación, métodos de compresión e incluso hardware específico; más recientemente, se han aplicado técnicas de procesamiento del lenguaje natural en aspectos tales como la extracción de información, la formulación de interrogaciones amigables y la generación de respuestas. Un componente importante de las técnicas de recuperación textual lo constituye la búsqueda de cadenas tanto exacta como aproximada -considerada por algunos autores tan básica como puedan serlo las operaciones aritméticas en otras áreas. También son importantes los métodos de construcción y manipulación de diccionarios -en general aceleran los procesos de búsqueda y reducen el tamaño de los índices. La construcción de diccionarios suele relacionarse con las técnicas de procesamiento del lenguaje natural. 
Un SRI permite la recuperación de la información, previamente almacenada (claro está), por medio de la realización de una serie de consultas ("queries") a los documentos contenidos en la base de datos. Esta serie de preguntas se conceptúan como sentencias formales de expresión de necesidades de información, y suelen venir expresadas por medio de un lenguaje de interrogación. Un documento es un objeto de datos, de naturaleza textual generalmente, aunque la evolución tecnológica ha propiciado la profusión de documentos multimedia, incorporándose al texto fotografías, ilustraciones gráficas, vídeo animado, audio, etc., 
Un SRI debe soportar una serie de operaciones básicas sobre los documentos almacenados en el mismo, como son: introducción de nuevos documentos, modificación de los documentos almacenados y eliminación de los mismos. Debemos también contar con algún método de localización de los documentos (o con varios generalmente), para presentárselos posteriormente al usuario. Los SRI implementan estas operaciones en formatos muy diversos, lo que provoca una amplia diversidad en lo relacionado con la naturaleza de los mismos, por lo que precisamos llegar a establecer una clasificación de los mismos.
1.2. Clasificación de los sistemas de recuperación de información. 
Como los SRI implementan una gama diversa de estructuras de datos, algoritmos y técnicas de recuperación de información, para facilitar su comprensión precisamos disponer de un marco conceptual para los mismos. Existe un método, denominado Domain Analysis (Análisis de Dominio), presentado por Prieto-Diaz y Arango (1), para desarrollar este marco. Este método permite diferenciar y recordar fehacientemente las similitudes y las diferencias entre estos sistemas, que se recogen en la siguiente tabla: 
Modelo Conceptual 
Estructura de Ficheros 
Operaciones de consulta 
Operaciones sobre términos 
Operaciones sobre documentos 
Booleano
Fichero Plano
Reutilización
Stemming
Visualización documentos
Booleano extendido
Fichero Inverso
Parsing
Ponderación por pesos
Rango
Probabilístico
Patrones de bits
Booleanas
Lista de palabras vacías
Ordenación
Búsqueda por cadenas
Árbol PAT
Clustering
Truncamiento
Enmascaramiento
Espacio Vectorial
Grafos
 
Indización
Asignación Ids 

La primera fila de la tabla recoge las facetas o atributos constantes en todos los sistemas. Las facetas representan las partes de los SRI que tienden a aparecer en todos los que se desarrollan, como es el caso de la estructura de la base de datos, que poseen todos los SRI aunque cada uno la defina de forma diferente. Con base en los distintos valores de las facetas podemos llegar a clasificar los SRI, así un sistema de recuperación de información que denomináramos genéricamente ALFA podría clasificarse de la siguiente manera: {ALFA }

Modelo conceptual
Estructura de ficheros
Operaciones de consulta
Operaciones sobre los términos
Operaciones sobre los documentos
Booleano
Fichero Inverso
Parsing, booleanas
Stemming,
Palabras vacías, Truncamiento
Parsing, Visualización docs, Ordenación. Enmascaramiento, Asignación Ids

Cada faceta refleja un momento determinado en la toma de decisiones efectuada para llevar a cabo el desarrollo de la arquitectura de un SRI. El diseñador del sistema debe elegir, para cada faceta, un valor determinado de entre las alternativas dispuestas.

1.3. Modelos Conceptuales de Recuperación de Información. 
Entendemos que un Modelo Conceptual de Recuperación de Información es una aproximación general a los SRI. Diversas taxonomías de modelos han sido propuestas, desde la clasificación de Faloutsos (2): búsqueda en patrones de texto, ficheros inversos y búsqueda en patrones de bits, hasta la clasificación de Belkin & Croft (3)que los dividen en exactos e inexactos. Los modelos exactos constituirían una categoría que incluiría a los modelos de búsqueda con base a patrones y a las técnicas booleanas; los modelos inexactos contendrían las técnicas probabilísticas, los espacios vectoriales y las técnicas de clustering, entre otros. Estas divisiones taxonómicas no resultan mútuamente excluyentes, y un SRI cualquiera puede llegar a presentar aspectos correspondientes a varias de ellas. 
La mayoría de los sistemas de información son de dos tipos, booleanos y de búsqueda de información por patrones de texto. Las interrogaciones a los sistemas de búsquedas por patrones de texto se llevan a cabo por medio de cadenas de caracteres o por expresiones regulares. Los sistemas de patrones de textos son más utilizados comúnmente en pequeñas colecciones de datos y cuando hay que gestionar grandes volúmenes de documentos destacan mayoritariamente los sistemas booleanos. Dentro de un sistema booleano, los documentos se encuentran representados por conjuntos de palabras clave, generalmente almacenadas en un fichero inverso. Un fichero inverso es una lista de palabras clave y de identificadores de los documentos en lo que éstas aparecen. Las búsquedas booleanas consisten en expresiones de palabras claves conectadas con algún/nos operador/es lógico/os (AND, OR y NOT). Al mismo tiempo que se han criticado estos sistemas (Belkin & Croft recogen un sumario de estas críticas), ha resultado complicado aumentar su efectividad en la recuperación de información. Sobre este modelo conceptual se han desarrollado algunas extensiones que se recogen bajo la denominación de modelo Booleano extendido. 
Se ha tratado también de mejorar el rendimiento de los SRI por medio del uso de la información procedente de la distribución estadística de los términos, en tanto que la frecuencia de aparición de un término en un documento o conjunto de documentos podría considerarse un dato relevante a la hora de establecer una interrogación a la base de datos. La distribución de frecuencias de un término se implementa dentro del contexto de algunos modelos estadísticos, como es el caso del modelo de Espacio Vectorial, el modelo Probabilístico o el modelo conceptual Clustering. Por medio del uso de modelos probabilísticos y de las distribuciones de frecuencia de los términos de la base de datos, es posible asignar una probabilidad de importancia (un peso) a cada documento dentro de un conjunto de documentos recuperados para ser ordenados posteriormente según un cierto orden de importancia (pertinencia). También resulta posible efectuar agrupaciones (clusters) de los documentos de la base de datos basándonos en los términos que contienen y recuperar información desde estos grupos de documentos por medio de algoritmos de ranking.   
1.4. Estructuras de ficheros. 
Una decisión fundamental a tomar durante el diseño de los SRI es qué tipo de estructura de ficheros se va a usar para la base de datos subyacente. En la tabla anterior hemos visto que el conjunto de estructuras de ficheros es diverso: ficheros planos, ficheros inversos, ficheros de patrones de bits, Árboles PAT y grafos. 
Con el uso de ficheros planos, uno o más documentos son almacenados en un fichero (generalmente en formato de texto ASCII), las búsquedas sobre estos ficheros planos se llevan a cabo generalmente por medio de la localización de patrones de texto. 
Un fichero inverso es un tipo de fichero índice donde la estructura de cada ítem (o entrada) del fichero es, generalmente: palabra clave, identificador de documento, identificador de campo. Una palabra clave es un término índice que describe al documento, el identificador de documento es único para cada documento y un identificador de campo es un término que nos indica dentro de qué campo del documento aparece la palabra clave. Algunos sistemas incluyen también información acerca de la localización en el documento del párrafo y frase de los términos utilizados para proceder a interrogar la base de datos. La búsqueda se realiza, corrientemente, por medio de la localización de los términos solicitados en el fichero inverso.
 
Los ficheros de patrones de bits contienen hileras de dígitos binarios, patrones de bits que representan a los documentos. Existen varias formas de construir estos patrones de bits, un método común consiste en la división de los documentos en bloques lógicos, conteniendo cada uno de ellos un número fijo de distinto significante (una palabra de una lista de términos no vacíos). Cada palabra del bloque es desglosada para ofrecer una hilera de bits (patrón de bits con algunos de los bits "puesto a 1"). Los patrones de bits de cada palabra en un bloque son agrupados para crear un bloque de patrones. Los bloques de signaturas se concatenan posteriormente para producir el patrón de bits del documento. La búsqueda se lleva a cabo por medio de la comparación que se establecerían entre los patrones de bits de las interrogaciones con los patrones de bits de los documentos de la base de datos. 
Los árboles PAT (de "PATricia trees"), están construidos sobre todas las sistrings de un texto (subcadenas). Si una colección de documentos es concebida como una secuencia numerada de arrays o cadenas de caracteres, una sistring se entiende como una subcadena de caracteres que se define desde un punto determinado del array y se extiende hasta una distancia arbitraria hacia la derecha. Un árbol PAT es un, por tanto, un árbol digital donde los bits individuales de las claves son usados para decidir derivaciones. 
Los grafos (o "redes"), son colecciones ordenadas de nodos conectados por arcos; se usan para representar documentos de diversas formas y maneras. Un ejemplo es el grafo denominado red semántica, que representa las relaciones semánticas que se establecen en el texto, y que se pierden (a menudo), en otros sistemas de indización. Aunque constituyen un campo interesante para el estudio, resultan bastante difíciles de llevar a la práctica y requieren excesivo esfuerzo manual para el proceso de la representación de las colecciones de documentos.  
1.5. Operaciones de consulta, operaciones sobre términos y operaciones con documentos. 
1.5.1. Operaciones de consulta. Las consultas a los SRI se expresan por medio de sentencias formales de las necesidades de información de los usuarios del sistema. Determinan de forma clara al SRI y permiten diferenciar a unos de otros. Por ejemplo, una de las operaciones de consulta más común es la operación denominada parsing, que consiste en la división de la consulta en sus elementos constituyentes. Las búsquedas booleanas deben ser divididas en sus correspondientes términos de indización o palabras clave y los operadores asociados a ellas para formular la expresión formal de la consulta. El conjunto de los documentos asociados con cada término de consulta es recuperado, y estos conjuntos son, entonces, combinados de acuerdo a los operadores booleanos. 
La operación denominada reutilización (en inglés feedback), consiste en la reutilización de una búsqueda anteriormente efectuada. La información sobre el resultado de estas búsquedas es usada para formar parte de las consultas actuales; así, los términos de documentos relevantes encontrados en una consulta previa pueden añadirse a la consulta actual, y los términos correspondientes a documentos no relevantes pueden ser obviados con el factor añadido de no tener que repetir las operaciones anteriores. 
1.5.2. Operaciones sobre los términos. Las operaciones que se pueden llevar a cabo sobre los términos en un SRI conforman el conjunto: {stemming, truncamiento, ponderación por pesos, palabras vacías y tesauros}. Con el concepto de stemming nos referimos a un proceso de "corte" de las palabras, reduciéndolas normalmente a su forma de raíz más común. 
El truncamiento es otro proceso de "corte de palabras" pero realizado de forma manual por el usuario en los procesos de recuperación de información, tal como puede ser la localización de todos los documentos que comiencen por "informa". 
Otra forma de asociación de términos relacionados es por medio de la utilización de un tesauro, el cual, nos va a ofrecer una lista de términos, sus términos sinónimos y las relaciones semánticas mantenidas entre los términos del mismo. La lista de palabras vacías es una relación de términos considerados como valores no indizables, usados para eliminar potenciales términos de indización. Los términos de una lista vacía están carentes de todo significado a la hora de recuperar información, como ejemplo podemos tomar el determinante "la", que no posee ninguna funcionalidad a la hora de recuperar documentos, ya que en todos los documentos de la base de datos aparecerá este término de forma casi segura y no nos resalta nada del contenido del documento almacenado. Así, cada término potencial de indización es comprobado previamente, verificándose su presencia en la lista de palabras vacías y es descartado si se encuentra en ella.
En cuanto a la ponderación de términos, a éstos se les puede asignar un valor numérico basado en su distribución estadística, o sea, en la frecuencia con la que los términos aparecen en documentos, colecciones de documentos, o en subconjuntos de colecciones de documentos, tales como documentos considerados relevantes en una búsqueda (pregunta).  
1.5.3. Operaciones sobre los documentos. Los documentos son los objetos primarios en un SRI y hay muchas operaciones para ellos. En algunos SRI, a los documentos añadidos a una base de datos se les debe asignar un identificador único, deben dividirse (en partes gramaticales) en sus campos constituyentes, y estos campos deben ser introducidos dentro de identificadores de campos y conjuntos de términos. Una vez en la base de datos, uno a veces quiere desenmascarar ciertos campos para buscarlos y mostrarlos, por ejemplo, un investigador puede desear buscar sólo los campos de título y resumen de un documento para una búsqueda dada, o puede desear consultar sólo el título y el autor de los documentos recuperados (4). 
Otra operación común es la de ordenar los documentos recuperados por algún campo determinado; por ejemplo el campo autor. La operación de mostrar incluye tanto a la salida impresa de los documentos como a su visualización en la pantalla del ordenador. A partir de la información procedente de la distribución de frecuencias de los términos, es posible asignar una probabilidad de relevancia a cada documento dentro de un conjunto recuperado, permitiendo que los documentos recuperados sean organizados en orden a esta probable relevancia. 
La información de la distribución de frecuencias de los términos puede ser usada para agrupar documentos similares en un espacio documental, por medio de las técnicas de clustering. Otra operación importante a realizar con los documentos es proceder a su visualización. El diseño del interfaz de usuario de un SRI resulta de carácter vital, como en otro tipo de sistemas de información, para conseguir un uso efectivo del mismo.  
1.6. Vista funcional del paradigma de un SRI.
En la siguiente figura se ilustra la actividad asociada con un tipo común de SRI basado en el modelo Booleano, el más empleado hasta ahora.
.  
   
Si estudiamos el proceso anterio
Si estudiamos el proceso anterior desde el extremo superior, es decir desde el punto de vista del documento que se introduce en el SRI van a ocurrir los siguientes procesos: 
A cada documento que entra se le asigna un Identificador 
Se identifican las palabras contenidas en el documento 
Se excluyen las palabras vacías 
Se "cortan" las palabras, es decir, se extraen las raíces de las palabras 
Se establece un peso de ponderación para cada raiz 
Finalmente las raíces debidamente ponderadas se introducen en la base de datos 
Cuando el usuario lleva a cabo una operación de recuperación de información, acaecerán los siguientes procesos: 
El usuario en función de sus necesidades y conveniencias lleva a cabo una serie de juicios de relevancia para confeccionar su ecuación de búsqueda, ayudándose de las prestaciones que le proporciona el Interfaz de Búsqueda. 
La ecuación de búsqueda, una vez introducidda, se descompone en sus partes fundamentales. 
Los términos clave empleados en la ecuación de búsqueda son "cortados" para extraer de ellos sus raíces y de esta forma proceder a su localización en la base de datos. 
Una vez localizados los distintos subconjuntos de documentos asociados a los términos clave, se llevan a cabo las operaciones booleanas pertinentes, que han sido introducidas por el usuario en la ecuación de búsqueda. 
Posteriormente los documentos pueden alinearse para su presentación según un ranking determinado. 
Otra vista lógica de la entrada de los documentos a los SRI nos la ofrecen Baeza-Yates y Riberiro-Neto (5): 
 

Donde podemos observar que se contempla un nuevo proceso posterior al de "corte" o extracción de las raíces de las palabras, como es el de la Indización (manual o automática), con esta operación, destinada a captar y representar el contenido de los documentos se persigue eliminar la presencia de términos ambiguos en los índices de las bases de datos, contribuyendo a la eficacia de su operatoria y a mejorar su consistencia.
 1.7. SRI y otras modalidades de sistemas de información. 
¿Cómo se relacionan los SRI’s con otros tipos de información tales como sistemas gestores de bases de datos relacionales (SGBDR) o sistemas de inteligencia artificial (IA)? En la siguiente tabla recogemos algunas de las similitudes y algunas de las diferencias: 
 Tipo de sistema 
Objetos de Datos 
Operaciones Primarias 
Tamaño de la Base de Datos 
SRI 
Documento 
Recuperación (Probabilística) 
pequeño a muy grande 
SGBDR (relacional) 
Tabla 
Recuperación (Determinística) 
pequeño a muy grande 
IA 
Afirmación Lógica 
Inferencia 
normalmente pequeño 
Una diferencia entre estos tres sistemas es el volumen de estructura usable en sus objetos de datos. Los documentos gestionados por un SRI, siendo, en general, primariamente de texto, poseen menos estructura usable que las tablas de datos gestionadas por un SGBDR, y estructuras tales como las redes semánticas usadas por un sistema IA. Es posible, por supuesto, analizar un documento manualmente y almacenar la información sobre su sintaxis y semántica en un sistema SGBDR o IA. La barrera para hacer esto en una amplia colección de documentos es más bien práctica que teórica. El trabajo involucrado en aplicar ingeniería del conocimiento sobre un conjunto de 50.000 documentos sería enorme. Los investigadores se han esforzado por construir sistemas híbridos, usando SRI, SGBDR, IA y otras técnicas. 
Otra característica distinta de un SRI es que la recuperación es probabilística. Esto es, uno no puede estar seguro de que en un documento extraído se recoge la información exacta que necesita el usuario. En una búsqueda típica en SRI, algunos documentos relevantes se pierden y otros no relevantes son recuperados. Esto puede ser contrastado con resultados de búsqueda de un SGBDR, donde la recuperación es determinística. En este caso, la pregunta consiste en un par de atributo-valor que coinciden o no con los registros en una base de datos. 
Un rasgo distintivo de los SRI, en comparación con muchos SGBDR, es que sus bases de datos son muy grandes (algunas veces en el rango de gigabyte). Las bases de datos de catálogos de bibliotecas, por ejemplo, pueden contener millones de registros. Los servicios comerciales de recuperación online (como Dialog o BRS) proporcionan a sus usuarios acceso en tiempo real a bases de datos de muchos gigabytes. La necesidad de buscar en colecciones tan grandes en tiempo real, genera múltiples demandas en el sistema usado para buscar en ellas. La selección de la mejor estructura de datos y de los algoritmos para construir tales sistemas es a menudo crucial. 
Otra característica de un SRI, que comparte con los SGBDR, es la volatilidad de los datos. Una colección de documentos gestionada por una típica aplicación SRI (como puede ser un sistema para bibliotecas o de consulta a servicios comerciales de recuperación de documentos), cambia constantemente mientras se vayan añadiendo, cambiando o borrando documentos. En resumen, un SRI representativo debe tener los siguientes requerimientos funcionales y no funcionales: 
 
Debe permitir a los usuarios añadir, borrar y cambiar documentos en la base de datos.
 
Debe proporcionar a los usuarios la manera de buscar documentos tecleando preguntas, y examinando los documentos recuperados.
 
Debe acomodar bases de datos en el rango de MB a GB
 
Debe permitir recuperar documentos relevantes en respuesta a preguntas interactivamente, en un tiempo de 1 a 10 segundos.
1.8. Web y S.R.I.
El rápido crecimiento del volumen de información disponible en Internet, ha hecho necesario mejorar los mecanismos de búsqueda y aprovechar al máximo las posibilidades que nos ofrece la propia Internet. Para ello es necesario conocer cómo funcionan los buscadores como herramienta básica, así como otros métodos para realizar búsquedas. Si se busca un documento en Internet que realmente existe, tarde o temprano lo encontraremos, pero se trata de encontrarlo en el menor tiempo posible. Un conocimiento de las características de los diferentes buscadores permite elegir el método de búsqueda en cada caso. Dentro de un mismo buscador, además, el resultado puede variar muchísimo, como veremos, refinando la interrogación a la base de datos. Por tanto, la idea principal es saber las posibilidades de los distintos buscadores, y establecer una comparación entre las características de cada uno de ellos. 
Un motor de búsqueda o mecanismo de búsqueda (search engine) es un programa que realiza búsquedas dentro de una base de datos. En el caso que nos ocupa, la base de datos es de recursos web. Un robot, según el WWW Robots FAQ, es un programa que de manera automática atraviesa la estructura de documentos Web extrayendo un documento y a partir de éste extrayendo recursivamente todos los documentos que está referenciados por enlaces. Los documentos son introducidos en una base de datos e indexados para su posterior localización por un mecanismo de búsqueda. Un índice o directorio es una recopilación manual de documentos, que pueden mantenerse como directorio o bien ser introducidos también en una base de datos para permitir que se realicen búsquedas. 
Los robots nacieron con la función de medir el tamaño del WWW, pero rápidamente se convirtieron en herramientas muy útiles para localizar documentos. El criterio para seleccionar las páginas que visita un robot depende de cada robot. En general parten de una lista de servidores inicial, y a partir de ahí va visitando los diferentes enlaces de cada página hasta un nivel abitrario respecto al inicial. Cuando un robot entra en un nuevo servidor, busca un fichero que se llama robots.txt, en el que se le indican los directorios permitidos y los prohibidos. Si este fichero no existe, considera todos permitidos. Además, se puede solicitar al robot direcciones de páginas para que sean visitadas e incluídas en la base de datos. Para esto se suele rellenar un formulario (submission form). La manera en que cada robot indexa el contenido de las páginas que visita también varía de unos a otros. Algunos robots indexan los títulos de páginas HTML, los primeros párrafos o el contenido entero del documento, etc. Últimamente se ha generalizado el uso del elemento HTML <META> (etiquetas META o metaetiquetas), que está oculto para el usuario, y que permite al creador de la página indicar al robot qué palabras clave quiere que sean indexadas y la descripción de la página que aparecerá cuando un usuario la localiza.
Los motores de búsqueda realizarán búsquedas dentro de una base de datos de documentos, que puede haber sido recopilada por un robot, o bien puede ser un índice recopilado manualmente. En cualquier caso, el motor de búsqueda recibe la interrogación del usuario (query), que consiste en una o varias palabras, realiza la búsqueda en la base de datos, y extrae una lista ordenada de documentos que cumplen entera o parcialmente con la interrogación. El orden depende de una puntuación (score) que asocia el programa a cada documento cuando realiza la búsqueda, y en cada caso varía. Normalmente se suelen tener cuanto antes aparecen las palabras, cuanto más juntas se encuentran entre sí, etc. 
Sustancialmente, las técnicas de recuperación de información empleadas por los motores de búsqueda en Internet, en un principio, derivaban de las empleadas tradicionalmente en el campo de los S.R.I. y es por ello que han comenzado a surgir grandes problemas cuando realizamos operaciones de recuperación de información con ellos, en tanto que el entorno de trabajo no es el mismo y las características intrínsecas de los datos almacenados en los mismos difieren considerablemente. Además, en el entorno web surgen problemas nuevos tal como es el caso del famoso fenómeno denominado "spamming", por medio del cual los constructores de páginas webs introducen en la descripción de las mismas términos que nada tienen que ver con el contenido de las mismas, por ejemplo: "mp3", "sex", "pamela anderson", "microsoft" (términos todos ellos de uso muy frecuente por todos aquellos usuarios de los motores de búsqueda) y que provocarán que estos usuarios recuperen esas páginas "trucadas" cuando ellos pretenden recuperar documentos de otra temática. Otro problema importante que afecta a la efectividad de estos sistemas reside en el enorme tamaño del índice, que poco a poco llega a alcanzar magnitudes impresionantes.
Si bien existen muchas diferencias entre estos ingenios de búsqueda, la mayor parte de los mismos emplean el Modelo Conceptual del Espacio Vectorial y muestran sus resultados ordenados según un algortimo de ranking, en el último par de años nos hemos encontrado con un nuevo concepto en este campo, se trata del motor de búsqueda denominado Google (6) desarrollado en la Universidad de Stanford en California, concebido para realizar un uso eficiente del espacio de almacenamiento y para proteger al índice de que se convierta en un elemento lento e inoperativo. 
1.8.1.Objetivos del diseño de Google. Hacia 1994 se pensaba que un índice completo de un motor de búsqueda permitiría encontrar cualquier documento de forma fácil, siempre que todos los documentos se encontraran incluidos en la base de datos. Sin embargo, pocos años después, la Web puede considerarse muy diferente en tanto que cualquier persona que lleve a cabo una búsqueda puede atestiguar fácilmente que lo completo que se encuentre el índice no implica necesariamente una alta calidad en los resultados de la búsqueda, de hecho, por desgracia se encuentra mucha más "basura documental" que documentos vinculados con nuestras necesidades de información. Incluso resulta difícil encontrarlos entre los veinte primeros documentos recuperados y la mayor parte de los usuarios no suelen pasar de esos primeros veinte documentos en sus operaciones de consulta (incluso algunos motores de búsqueda sugieren que en caso de no encontrar nada en ese rango procedamos a modificar nuestra ecuación de búsqueda). Es por ello que el objetivo primordial de diseño de Google no es otro que mejorar estos índices de precisión en la recuperación de la información y, además, mejorar la presentación de los documentos recuperados de manera que, los primeros sean los más directamente relacionados con las necesidades de información planteadas por los usuarios.
1.8.2. Características de Google. Destacan dos grandes características, en primer lugar hace uso de la conectividad de la Web para calcular un grado de calidad de cada página, esta graduación se denomina "PageRank" (coincide con el nombre del algoritmo de ranking empleado por este motor). En segundo lugar, Google utiliza esta propia capacidad de conexión de los documentos webs para mejorar los resultados de búsqueda. Pagerank asume que el número de enlaces que una página proporciona tiene mucho que ver con la calidad de la misma, es por ello que este algoritmo se puede resumir de la siguiente manera:
"Si una página A tiene T1....Tn páginas que apuntan a ella por medio de algún enlace (es decir citas). El parámetro d es un factor que se puede fijar entre 0 y 1 (generalmente se fija en 0.85) . Sea C(A) es número de enlaces que salen de la página A. Entonces, el PageRank de la página A vendrá dado por la expresión: PR(A) = (1-d) + d(PR(T1)/C(T1) + ..... + PR(Tn)/C(Tn))"
Este cálculo puede realizarse por medio de un simple algortimo iterativo y corresponde al vector propio de una matriz normalizada de enlaces en la Web. PageRank puede ser pensado como un modelo del comportamiento del usuario. Si asumimos que hay un "navegante aleatorio" que pasa de una página a otra sin presionar nunca el botón de "back" y que, eventualmente él nunca se aburriera, la probabilidad de que este navegante visitara nuestra página es precisamente su PageRank. Es decir, se trata de un modelo basado en los enlaces de las páginas pero que pretende representar la forma de trabajar de los usuarios. Otra justificación intuitiva de PageRank es que una páigna puede tener un alto coeficiente de PageRank si existen muchas páginas que apuntan a ella, o si hay un número algo menor de páginas que apuntan a ella pero que posean, a su vez, un alto nivel de PageRank. De forma intuitiva, aquellas páginas muy citadas son páginas que vale la pena consultar y, en cambio, aquellas que sólo posean un enlace son páginas de poco interés para su consulta. 
1.8.3. Vista general de la Arquitectura de Google. Google posee varios sistemas para llevar a cabo su tarea. En Google el análisis de las páginas web se lleva a cabo por medio de diversos procesos distribuidos. Hay un URLServer que envía lístas de de direcciones URLs para ser analizadas por los agentes. Estas páginas, una vez analizadas, son enviadas al Servidor de Almacenamiento (StoreServer). Este nuevo elemento se encarga de comprimir y almacenar las páginas web. Cada página web va a tener asociado un identificado numérico (ID number) denominado "docID". La función de indización (o indexación) la llevan a cabo el Indexador y el Clasificador (Indexer y Sorter). El primero lleva a cabo las siguientes funciones: lee las páginas web procedentes del StoreServer, descomprime los documentos y selecciona los términos incluidos en los mismos. Cada documento se convierte en un conjunto de palabras, que van a denominarse "hits", donde grabamos loa palabra y su posición en el documento, una aproximación de su fuente de texto y otra serie de detalles. El Indexador analiza los enlaces incluidos en cada página web y almacena una información muy importante acerca de ellos en un Fichero de Enlaces. Este fichero contiene suficiente información para determinar hacia dónde apunta cada enlace y el texto de cada enlace. 
. 
 
El componente URLresolver lee el Fichero de Enlaces y convierte las URLs relativas en direcciones absolutas convirtiéndolas en identificadores docsIDS. Así, genera una base de datos de pares de docsIDs necesaria para que el Algoritmo PageRank pueda calcular los pares de valores de páginas enlazadas desde una página y de páginas que apuntan a esa página.
Tanto el URLresolver como el Indexador han generado unos depósitos de datos que Google denomina "barriles" (traducción literal de "barrels"). Estos depósitos de datos, ordenados por docID son leidos por el Clasificador y recurre a ellos para generar el Fichero Inverso. Esta operación se lleva a cabo empleando una porción muy pequeña de espacio temporal, por lo que no consume apenas recursos del sistema. También construye una lista de wordIDs (identificadores de términos) y de ubicaciones en la página de cada término dentro de este Fichero Inverso que va a ser leído por una aplicación denominada DumpLexicon que, junto al conjunto de datos producidos por el Indexador genera un nuevo léxico que va a ser utilizado por el Localizador (Searcher), junto a los datos provenientes del PageRank.
1.8.4. ¿Cómo busca información Google? Hemos visto, a un nivel muy general, cómo busca información Google, vamos ahora a estudiar, también a un nivel muy general, cómo localiza la información. Para ello debemos recordar que el objetivo de la búsqueda no es otro que proporcionar una alta efectividad, y que el usuario lo primero que percibe es la precisión de los resultados de la búsqueda. El proceso de evaluación de la pregunta que lleva a cabo Google es el siguiente: 
Descomposición (parsing) de la pregunta. 
Conversión de las palabras a wordIDS (identificadores de palabras) 
Localización de la posición de cada palabra en el barril de almacenamiento 
Exploración de las listas de documentos hasta localizar un documento que contenga todos los términos de búsqueda 
Cálculo del rango de este documento para esta pregunta 
Una vez llegados al final del barril de almacenamiento, se vuelve al inicio repitiendo los pasos 4 y 5 para cada palabra de la ecuación de búsqueda 
Una vez calculados todos los rangos, procede ordenarlos de mayor a menor y presentarlos al usuario. 
1.8.5. Más allá de Google. Aunque se trata del motor de búsqueda de moda y pasa por ser el más completo de todos los que funcionan hoy en día en la Web, ya encontramos aportacines de algunos autores que critican parcialmente el diseño de Google y propugnan otros diseños más eficientes según su punto de vista. Este es el caso de Zhang y Dong (7) quienes propugnan diseñar los algoritmos de ranking a partir de la sinergia de las siguientes métricas: 
 
Relevancia: métrica empleada por la mayor parte de los motores de búsqueda basados en el modelo del espacio vectorial. Mide la distancia entre el contenido de un recurso web (r) y una pregunta de un usuario cualquiera (q).
 
Autoridad: cuántos recursos webs poseen enlaces al recurso r.
 
Integración: mide cuántos recursos webs son enlazados por el recurso r
 
Novedad: en qué grado el recurso r es diferente de otros y proporciona información nueva
1.9. Evaluación de los SRI. 
1.9.1. Criterios para la evaluación. Un SRI puede ser evaluado por diversos criterios, incluyendo entre los mismos los criterios de: eficaz ejecución, efectivo almacenamiento, efectividad en la recuperación y la serie de características que ofrece al usuario. La relativa importancia de estos factores debe ser decidida por el diseñador del sistema, y la selección de la apropiada estructura de datos y algoritmos para su implementación dependerá de esa decisión. 
La eficacia en la ejecución es medida por el tiempo que se toma un sistema o una parte de un sistema para realizar una operación. Este parámetro ha sido siempre la preocupación principal en un SRI, especialmente desde que muchos de ellos son interactivos, y un largo tiempo de recuperación interfiere con la utilidad del sistema, llegando a alejar a los usuarios del mismo. Los requerimientos no funcionales de un SRI normalmente especifican el máximo tiempo aceptable para una búsqueda, y las operaciones de mantenimiento de una base de datos tales como añadir y borrar documentos. 
La eficiencia del almacenamiento es medida por el número de bytes que se precisan para almacenar los datos. El espacio general, una medida común de medir la eficacia del almacenamiento, es la razón del tamaño del índice de los ficheros además del tamaño de los archivos del documento sobre el tamaño de los archivos del documento. Las ratios del espacio general que oscilan entre los valores 1,5 y 3 son típicas de los SRI basados en los ficheros inversos. 
De forma tradicional se ha conferido mucha importancia a la efectividad de la recuperación, normalmente basada en la relevancia de los documentos recuperados, lo cual ha representado un problema ya que medir esa relevancia es un proceso altamente subjetivo y sin confianza. Esto se debe a que diferentes juicios personales asignarían diferentes valores de relevancia a un documento recuperado en respuesta a la búsqueda hecha. La seriedad del problema es la materia de debate, bastantes investigadores señalan que la subjetividad del juicio sobre la relevancia no es suficiente para invalidar el sistema. Muchas medidas de la efectividad de la recuperación han sido propuestas. Las más empleadas, de forma general, son las conocidas como exhaustividad (recall) y precisión. 
Exhaustividad es la ratio de documentos relevantes recuperados en una búsqueda dada, sobre el número de documentos relevantes para esa búsqueda contenidos en la base de datos. Excepto para tests realizados sobre pequeñas colecciones, este denominador es generalmente desconocido y debe ser estimado por muestreo o por otros métodos. Precisión es la ratio del número de documentos relevantes recuperados, sobre el número total de documentos recuperados. El rango de valores de ambas ratios, está comprendido entre 0 y 1. 
  
 

En tanto que, generalmente, se quiere comparar la realización del SRI en los términos de exhaustividad y precisión, se han desarrollado métodos para evaluarlos de forma simultánea. Un método comprende el uso de grafos de exhaustividad-precisión -puntos bivariados, donde un eje es para la exhaustividad y otro para la precisión. La figura anterior muestra un ejemplo de tales puntos que están inversamente relacionados. Esto es, cuando la precisión sube, la exhaustividad normalmente baja y viceversa. Una medida de evaluación combinada de rellamada y precisión, E, ha sido desarrollada por Van Rijsbergen (8) y definida como: 
  E = 1 - [(1 + b2) P R / (b2 P +`R)] 
Donde {P = precisión, R = exhaustividad}, y b es una medida de la importancia relativa, para un usuario, de exhaustividad y precisión. Los investigadores eligen valores de E que ellos esperan que reflejarán la rellamada y precisión que interese al usuario típico. Por ejemplo, si los valores de b se encuentran en niveles de 0.50, nos indica que un usuario estuvo dos veces tan interesado en la precisión como en la rellamada, y si el valor de b fuera 2, nos indica que un usuario estuvo tan interesado en la rellamada como en la precisión. 
1.9.2. Propuesta tradicional de Lancaster (1973). Los criterios indicados anteriormente no resultan los únicos que se han propuesto, de hecho, casi desde el desarrollo de los primeros sistemas se vienen proponiendo otra serie de criterios, tal como es la propuesta de Lancaster quien establece seis criterios para la evaluación de los sistemas de recuperación de información: 
Cobertura 
Exhaustividad 
Precisión 
Tiempo de Respuesta 
Esfuerzo del usuario 
Formato 
Aunque estos criterios tienen casi tres décadas de antigüedad, algunos de ellos continúan perfectamente vigentes hoy en día e incluso hay que apuesta por su aplicación para evaluar las prestaciones de los distintos motores de búsqueda en Internet.
1.9.3. Propuesta de evaluación de los motores de búsqueda en la web. Es en esa línea de trabajo donde nos encontramos a Chu y Rosenthal, autores quienes consideran necesario establecer una metodología para poder evaluar debidamente la calidad de los motores de búsquedaen la web, ya que su propia naturaleza propicia que los considerandos tradicionales no terminen de servirnos en este nuevo contexto.
En este campo de trabajo se han llevado a cabo muchos y diferentes procesos de evaluación, algunos de ellos pueden considerarse "superficiales" en tanto en basan sus conclusiones en las características que los propios administradores de estos motores proporcionan a través de sus webs y no realizan ningún tipo de test o experimentación con los mismos. Otros trabajos sí llevan a cabo experimentaciones, muchas de ellas relacionadas con el cálculo tradicional de la precisión de los resultados recuperados y otros van algo más allá, considerando medidas tales como el tamaño del índice de cada motor o el período de actualización de este índice. Es por ello que aunque encontremos una abundante literatura científica se hace precisa una sintetización de la misma y un análisis comparativo de los resultados proporcionados.
Como ejemplo de lo anterior, nos encontramos con que Courtois, Baer y Stark (1995) aseguran que OpenText es el mejor de los motores de búsqueda gracias a su flexibilidad, fortaleza de sus búsquedas y el rápido tiempo de respuesta, asegurando también que WebCrawler es el más adecuado para los novatos. Scoville (1996) apuesta por Excite, Infoseek y Lycos. Leighton (1995) es de los pioneros en considerar necesario medir la precisión de las búsquedas, que además considera variables en el tiempo por la propia evolución de los índices. En su estudio clasifica como mejores a Lycos e Infoseek (aunque el estudio sólo abarca a cuatro motores). Kimmel (1996) califica a Lycos como el mejor de un conjunto de ocho motores evaluados, aunque se limita a características externas o superficiales. Un estudio más amplio fue llevado a cabo por C¡Net, asesor de evaluación online, que evaluaron a 19 motores de búsquedas destacando a Altavista como el mejor de todos, tendencia que es seguida por otros muchos estudios, siendo uno de los más completos el realizado por Gordon y Pathak (1999), donde se evaluó con mucho rigor a ocho motores de búsqueda. Resulta curioso, no obstante, observar una amplia diversidad de opiniones en el período inicial de existencia de estos ingenios de búsqueda y una cierta unanimidad a medida que ha ido pasando el tiempo hacia la idea de cierta superioridad de Altavista, Opentext y Lycos sobre los demás. También hay que considerar que todos estudios son previos a la aparición de Google por lo que precisan de revisión.
Es por ello que se hace preciso establecer una metodología para el análisis de estos motores de búsqueda, de manera que cuando se lleven a cabo nuevos estudios, los resultados procedentes de los mismos no resulten muy dispares y permitan a los estudiosos del tema extraer unas conclusiones acertadas sobre la calidad de los mismos.
Chu y Rosenthal (9) apuestan por la vigencia de los postulados de Lancaster, ya que consideran a los motores de búsqueda como sistemas de recuperación de información caracterizados por un enorme tamaño, por su estructura hipertexto y por su arquitectura distribuida, pero perfectamente susceptibles de ser evaluados con estos criterios. Tomando como base los postulados de Lancaster dedujeron las siguientes conclusiones: 
  
 
Tiempo de respuesta: los distintos motores evaluados (Altavista, Excite y Lycos) no varían sustancialmente con respecto a este parámetro.
 
Precisión: se estudiaron las diez primeras respuestas y en este caso Altavista alcanzaba valores cercanos al ochenta por ciente de precisión cuando los otros dos rondan el cincuenta por ciento.
 
Capacidades de búsqueda: Altavista incorpora operadores de proximidad, los otros no. La gestión de las frases literales también es mejor en Altavista y Lycos supera a los otros dos en el caso del truncamiento, aunque esta operación de recuperación de información, debido a su carácer exhaustivo incorpora ruido documental.
 
Formatos: cada uno de ellos presenta ventajas e inconvenientes frente a los demás, Excite si lleva a cabo un resumen automático de la página web y lo presenta.
 
Documentación e Interface: en este caso los autores piensan que es Lycos el mejor.
Chu y Rosenthal terminan proponiendo el desarrollo de esa metodología que permita evaluar de forma coherente a los distintos motores de búsqueda, en ella se han de considerar cuantro conceptos generales: 
  
Composición de los índices: la composición de los índices afecta de forma muy directa a la calidad de la recuperación de información. Destacan tres componentes importantes: Cobertura, Frecuencia de Actualizacion y Porción de página web indexada (título, título y primeros párrafos, página completa, etiquetas meta). Las magnitudes de cada motor dependerá del Hw y Sw dedicado. De otro lado, el que el índice sea muy extenso no implica calidad (el ejemplo lo tenemos en Yahoo) y, de otro lado, el poseer un valor muy alto en este parámetro tampoco implica altos niveles en los otros tres. 
Capacidades de búsqueda: un motor ha de poseer operadores booleanos, búsquedas por expresiones literales, truncamiento de los términos y facilidades de acotar una búsqueda en un determinado campo. Deheho, este conjunto de prestaciones comienzan a considerarse básicas, destacando (negativamente) motores tan famosos como Lycos o Excite que no incluyen la búsqueda por frase literal. 
Ejecución de la recuperación de información: suele medirse con base en tres parámetros: Precisión, Exhaustividad y Tiempo de Respuesta. Estas medidas, no obstante, incorporan muchas dosis de subjetividad a la hora de determinar cuándo un resultado es relevante o no. 
Esfuerzo del usuario: la Documentación y el Interface son elementos a considerar en este apartado y suelen tener un aceptable nivel. De heco, este parámetro es muy importante porque un usuario no va a hacer uso de un motor si no se encuentra cómo con su interface, si no localiza fácilmente la documentación que indique cómo emplearlo y si no la comprende. 
Notas. 
1. PRIETO-DIAZ, R. and ARANGO, G. Domain Analysys: Acquisition of Reusable Information for Software Construction. New York: IEEE Press, 1991. 
2. FALUOTSOS, C. “Acces Method for Text”. En: Computing Surveys, vol 17, n.1. 1985. p 49-74 
3. BELKIN, N.J. and CROFT, W.B. “Retrieval Techniques”. En: Willians, M. (ed) Annual Review of Information Science and Technology. New York: Holt, Rinehart and Winston, 1986. 
4. En algunos sistemas gestores de bases de datos documentales, a este tipo de operación se le denomina búsqueda por referencia cualificada. 
5. BAEZA-YATES, R. and RIBEIRO-NETO, B. Modern Information Retrieval. Maryland: Addison-Wesley-Longman Publishing co, 1999. 
6.  BRIN, S. and PAGE, L. The anatomy of a large-scale hypertextual Web search engine. Computer Netsorks and ISDN Systems, 30, 1998. p. 107-117 
7. ZHANG, D. and DONG, Y. "An efficient algorithm to rank web resources". En <http://www9.org/w9cdrom/251/251.html>, 1999 [Consultado: 14 de agosto de 2000] 
8. VAN RIJSBERGEN, C.J. Information Retrieval. London: Butterworths, 1979. 
9. CHU, H. and ROSENTHAL, M. "Search engines for the WWW: A comparative study and evaluation methodology" En <http://www.asis.org/annual-96/ElectronicProceedings/chu.html>,1996.   [Consultado: 14 de septiembre de 2000] 

