Indexando la Web 
Queda claro que para extender un directorio como Yahoo! se necesitan expertos que clasifiquen nuevas páginas que en general son informadas por los propios interesados. Por otra parte, indexar toda la Web implica el uso de programas llamados crawler, robot, wanderer, etc. que recorren la Web y recopilan páginas nuevas o actualizadas. La arquitectura típica de un buscador (ver figura siguiente) incluye el indexador y el robot. A continuación hablamos de cómo crear un índice de toda la Web. 
 
Nadie conoce el volumen actual de la Web. Tratemos de subestimar la cantidad de texto existente en la Web. Si cada página tiene 5Kb y hay como 1500 millones de páginas, estamos hablando de más de 7.5 Tb de texto solamente. Esta es una estimación conservadora y por supuesto el volumen total es mayor. Indices como AltaVista mantienen todas las palabras distintas ordenadas y para cada palabra la lista de páginas Web donde aparecen. Esta estructura de datos se llama archivo invertido. 
El número de palabras distintas no crece en forma proporcional al texto, sino que crece en forma sublineal (crece como nx con 0<x<1). Esto se debe a que el vocabulario es finito y entonces muchas palabras se repiten. Por otra parte, la frecuencia de las palabras sigue una variante de la Ley de Zipf que caracteriza la ocurrencia de palabras en el texto. Esta ley experimental indica que la j-ésima palabra más frecuente aparece una cantidad de veces proporcional al inverso de j. Actualmente esta distribución es más sesgada y se aproxima más al inverso del cuadrado de j. Es decir, hay un conjunto pequeño de palabras muy frecuentes y muchas que aparecen muy pocas veces o sólo una vez (sea cual sea el idioma usado). 
Usando distintas técnicas, el tamaño de un archivo invertido puede reducirse a un 20% del tamaño del texto. Estos índices se pueden reducir usando particiones lógicas en vez de documentos (por ejemplo, poniendo muchas páginas pequeñas en un mismo grupo). Usando una búsqueda eficiente en las palabras ordenadas, podemos encontrar todos los documentos en que aparece en menos de un segundo. Dependiendo del sistema de búsqueda, estos documentos serán ordenados usando distintos criterios y heurísticas, con el objeto de indicar al usuario cuál es el documento más relevante (esto funciona muchas veces, pero otras no). Otro problema debido al volumen de datos es que la cantidad de documentos resultantes es del orden de miles, por lo cual es necesario usar paradigmas visuales para poder manipularlos. Por ejemplo, el índice de AltaVista, que es uno de los más grandes, registra sobre 300 millones de páginas Web, y para atender las consultas se usan decenas de servidores Alpha, cada uno con varios procesadores y 8Gb de memoria RAM (sí, leyó bien, 8Gb). Por lo tanto, gran parte del índice y muchas de las respuestas están almacenadas ya en RAM (para poder rápidamente retornar la siguientes 20 páginas de una consulta). Los otros buscadores con un número similar de páginas son Fast e Inktomi. Este último recientemente dice haber llegado a los 500 millones de páginas, que significaría una cobertura de alrededor de un tercio de la Web. Este esquema centralizado tiene un límite si la Web sigue creciendo como hasta ahora y el final de los buscadores existentes hoy en día podría ocurrir en un futuro cercano. 
Resultados recientes demuestran que el número de páginas que están en los buscadores más grandes es pequeño (del 2% al 5%) y en general se encuentran páginas distintas en cada uno de ellos. Por lo tanto, un buen metabuscador (buscador que busca en muchos buscadores) puede ser muy efectivo si sabe combinar y clasificar bien las respuestas. Otras ideas recientes incluyen agentes de software especializados o metabuscadores en temas específicos, por ejemplo Search Broker o Meta Miner. 
Un problema técnico importante es como jerarquizar las páginas. La mayoría de los buscadores usan la ocurrencias de las palabras que estamos buscando, pero esto muchas veces no funciona. Nuevas técnicas incluyen información de los enlaces, lo que es muy efectivo. Un buscador que usa esta idea es Google. Otro peligro es que los buscadores de Internet estén jerarquizando las respuestas en base a razones económicas y no de contenido. Esto es difícil de demostrar, pero algo igualmente preocupante se muestra en [3] acerca de este tipo de inequidad. Es mucho más probable que una página popular esté en el índice de un buscador como Altavista que una que no lo sea. Una forma de medir la popularidad es contar el número de enlaces dirigidos a una página. La figura adjunta muestra como la probabilidad de encontrar una página en un buscador aumenta dependiendo del número de enlaces que te dirigen a ella (en vez de ser constante). 
 
En este sentido NorthernLight (www.nlsearch.com) es el más equitativo. Así es que ya sabe, pídale a sus amigos que lo apunten y aparecerá en más buscadores. 
Epílogo 
La Web es un gran repositorio de datos y un nuevo medio de publicación al alcance de más de 100 millones de personas. El hacer uso eficiente y adecuado de estos datos depende de nosotros y de las herramientas que existen y que han sido descritas en este artículo. El futuro dirá si es posible adaptar estas herramientas al crecimiento explosivo de la Web y que además la Web misma no colapse debido a la congestión en las redes y servidores Web. Para mayor información en este tema, ver el capítulo 13 de Modern Information Retrieval [2]. 
En al ámbito Iberoamericano, seguirán apareciendo más buscadores, pues una solución al problema es verticalizar la búsqueda. Una forma de hacer esto es geográficamente por países o también culturalmente por idiomas. 